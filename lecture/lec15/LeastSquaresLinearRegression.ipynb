{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least Squares Linear Regression\n",
    "\n",
    "In this lecture we will introduce **linear models** and **least squares linear regression**.  We will explain the basic *parametric model* formulation and how to optimize it using **linear algebra** and **geometry** rather than gradient descent.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports \n",
    "\n",
    "In this notebook we will be using the following packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.offline as py\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.figure_factory as ff\n",
    "import cufflinks as cf\n",
    "cf.set_config_file(offline=True, sharing=False, theme='ggplot');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support embedding YouTube Videos in Notebooks\n",
    "from IPython.display import YouTubeVideo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Note on Lecture Format\n",
    "\n",
    "This is the first of a series of lectures that are designed to be completed online rather than in class.  This new lecture format is experimental and we ask for your feedback on Piazza. \n",
    "\n",
    "The playlist for all the videos in this lecture can be found here:\n",
    "\n",
    "[Least Squares Linear Regression](https://www.youtube.com/playlist?list=PLcK2S75CXo8MNPMb80AZOfwCFfWIdQFPV)\n",
    "\n",
    "### Intended Use\n",
    "We have designed this video notebook to be a series of short videos followed by simple questions (with answers provided) and demos. Note that all the solutions are provided and there are no grades for working through this notebook.  However, we hope that by combining videos with activities this will actually be more engaging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap\n",
    "\n",
    "The following short video provides a recap of the modeling recipe (model, loss, optimize).  If you are comfortable with this you can safely skip this video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YouTubeVideo(\"u0YycwQ814c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap Question\n",
    "\n",
    "What are the three stages of modeling used in this class?\n",
    "\n",
    "<details>\n",
    "    <summary>Answer</summary>\n",
    "    In this class, the recipe for modeling consists of defining the model, defining the loss, and optimizing the model with respect to the loss.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilinear Regression Setting\n",
    "\n",
    "In the following video we introduce the multiple linear regression setting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YouTubeVideo(\"21PN6a3s22I\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Diamonds Dataset\n",
    "\n",
    "To motivate the multilinear regression setting we will work with the classic diamonds dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/diamonds.csv.zip\", index_col=0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each record in the dataset corresponds to a single diamond.  The fields are\n",
    "\n",
    "1. **carat**: The weight of the diamonds\n",
    "2. **cut**: The quality of the cut is an *ordinal* variable with values (`Fair`, `Good`, `Very Good`, `Premium`, and `Ideal`)\n",
    "3. **color**: The color of the diamond is an *ordinal* variable with values `J` (worst) to `D` (best).\n",
    "4. **clarity**: How obvious inclusions are within the diamond. This is an *ordinal* variable with values `I1` (worstt), `SI2`, `SI1`, `VS2`, `VS1`, `VVS2`, `VVS1`, `IF` (best)\n",
    "5. **depth**: The height of a diamond, measured from the culet to the table, divided by its average girdle diameter.\n",
    "6. **table**: The width of the diamond's table expressed as a percentage of its average diameter.\n",
    "7. **price**: Price of the diamond in USD.\n",
    "8. **x**: length measured in mm\n",
    "9. **y**: width measured in mm\n",
    "10. **z**: depth measured in mm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in **predicting the price of a diamond given it's characteristics**.  Therefore, we would like to fit a model:\n",
    "\n",
    "$$\n",
    "f_\\theta\\left(\\text{Diamond's Characteristics}\\right) \\rightarrow \\text{Price}\n",
    "$$\n",
    "\n",
    "Suppose for now we focus only on the diamond's **carat**, **depth**, and **table** values.  In this case, we would want a model of the form:\n",
    "\n",
    "$$\n",
    "f_\\theta\\left(\\textbf{carat}, \\textbf{depth}, \\textbf{table}\\right) \\rightarrow \\textbf{price}\n",
    "$$\n",
    "\n",
    "We could express this as a linear model of the form:\n",
    "\n",
    "$$\n",
    "f_\\theta\\left(\\textbf{carat}, \\textbf{depth}, \\textbf{table}\\right) \n",
    "=\n",
    "\\theta_0 + \n",
    "\\theta_1 * \\textbf{carat} +\n",
    "\\theta_2 * \\textbf{depth} +\n",
    "\\theta_3 * \\textbf{table} \n",
    "$$\n",
    "\n",
    "Where $\\theta_0$ is the **intercept** parameter and $\\theta_1$, ..., $\\theta_3$ are the **\"slopes\"** with respect to each of the covariats.  \n",
    "\n",
    "Focusing on just the **carat**, **depth**, and **table** features and the price our dataset looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['carat', 'depth', 'table', 'price']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In lecture, we saw that the predictions for the entire data set $\\hat{\\mathbb{Y}}$ can be computed as:\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbb{Y}} = \\mathbb{X} \\theta  \n",
    "$$\n",
    "\n",
    "The **covariate matrix** $\\mathbb{X} \\in \\mathbb{R}^{n,d+1}$ has $n$ rows corresponding to each record in the dataset and $d+1$ columsn corresponding to the origina $d$ columns in the data plus an additional 1s column vector.  For example, the first five rows of $\\mathbb{X}$ for this dataset would look like:\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>0</th>\n",
    "      <th>1</th>\n",
    "      <th>2</th>\n",
    "      <th>3</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>1.0</td>\n",
    "      <td>0.23</td>\n",
    "      <td>61.5</td>\n",
    "      <td>55.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>1.0</td>\n",
    "      <td>0.21</td>\n",
    "      <td>59.8</td>\n",
    "      <td>61.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>1.0</td>\n",
    "      <td>0.23</td>\n",
    "      <td>56.9</td>\n",
    "      <td>65.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>3</th>\n",
    "      <td>1.0</td>\n",
    "      <td>0.29</td>\n",
    "      <td>62.4</td>\n",
    "      <td>58.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>4</th>\n",
    "      <td>1.0</td>\n",
    "      <td>0.31</td>\n",
    "      <td>63.3</td>\n",
    "      <td>58.0</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extract our covariate matrix and add a column of all 1s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n,d = data[['carat', 'depth', 'table']].shape\n",
    "print(\"n = \", n)\n",
    "print(\"d = \", d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.hstack([np.ones((n,1)), data[['carat', 'depth', 'table']].to_numpy()])\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define the linear model in python using numpy.  Here I will assume the input is in row vector form so this function will work with a single row or the entire matrix.  Note in lecture we discuss the case where a single row is stored in column vector form (as this is the case in most math textbooks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model(theta, xt): \n",
    "    return xt @ theta # The @ symbol is matrix multiply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we guess a value for $\\theta$ we can make a prediction using our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_guess = np.random.randn(d+1, 1)\n",
    "theta_guess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a single prection for the 3rd row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model(theta_guess, X[2,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a prediction for all the rows at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model(theta_guess, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copying the notation directly from the video lecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_hat = X @ theta_guess\n",
    "Y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining and Minimizing the Loss\n",
    "\n",
    "The next video defines and minimizes the least squares loss by using a geometric argument.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YouTubeVideo(\"zkJ3CULN8Wk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Least Squares Loss\n",
    "\n",
    "The Least Squares Regression loss is simply the average squared loss.\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\frac{1}{n}\\sum_{i=1}^n \\left( \\mathbb{Y}_i - \\mathbb{X}_i \\theta \\right)^2\n",
    "$$\n",
    "\n",
    "Here $\\mathbb{Y}_i$ is the $i$th observed response (diamond price) and $\\mathbb{X}_i$ is the **row vector** coresponding to the $i$th row of the covariates with an added 1 at the beginning. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define the response vector $\\mathbb{Y}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y =  data[['price']].to_numpy()\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Directly writing the **average squared loss**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_loss(theta):\n",
    "    return np.mean((Y - X @ theta)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squared_loss(theta_guess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using matrix notation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_loss(theta):\n",
    "    return ((Y - X @ theta).T @ (Y - X @ theta)).item() / Y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squared_loss(theta_guess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimizing the Loss\n",
    "\n",
    "In the video lecture we defined the normal equation as:\n",
    "\n",
    "$$\n",
    "\\mathbb{X}^T \\mathbb{X} \\hat{\\theta} = \\mathbb{X}^T \\mathbb{Y}\n",
    "$$\n",
    "\n",
    "If we solve for $\\hat{\\theta}$ this is the minimizer of the squared loss with respect to our data.  \n",
    "\n",
    "If we assume $\\mathbb{X}^T \\mathbb{X}$ is invertible (full rank) then we get the following solution:\n",
    "\n",
    "$$\n",
    " \\hat{\\theta} = \\left( \\mathbb{X}^T \\mathbb{X} \\right)^{-1} \\mathbb{X}^T \\mathbb{Y}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimizing the Loss\n",
    "\n",
    "The following video provides a better intuition of the shape of each of the terms in the solution to the normal equation as well as how to better solve it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YouTubeVideo(\"9vWR-19WQKU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it is not as numerically stable or efficient. We can compute $\\hat{\\theta}$ by direction using matrix inversion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import inv, solve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_hat = inv(X.T @ X) @ X.T @ Y\n",
    "theta_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_hat = solve(X.T @ X, X.T @ Y)\n",
    "theta_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solve can be implement slightly more efficiently by avoiding the inversion and subsequent matrix multiply.  For matrices which are less well behaved, solve can also be more numerically stable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing a Random Guess and Our Solution\n",
    "\n",
    "We can compare the $\\hat{\\theta}$ to our earlier random guess.  Notice here I will take the square root to make the loss in units of US dollars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"theta_hat\", np.sqrt(squared_loss(theta_hat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"theta_guess\", np.sqrt(squared_loss(theta_guess)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better than random guess but not by much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions\n",
    "\n",
    "Now that we have estimated the model parameters $\\hat{\\theta}$ we can now also predict the price $\\hat{\\mathbb{Y}}$ for each of our diamonds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_hat = X @ theta_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnosing the Model\n",
    "\n",
    "In previous lectures we have plotted the residual.  We can do that for each of the dimensions but in general it will be difficult to visualize the residual directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(x = X[:,1], y = Y - Y_hat,opacity=0.2)\n",
    "fig.update_xaxes(title=\"Carat\")\n",
    "fig.update_yaxes(title=\"Residual\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(x = X[:,2], y = Y - Y_hat,opacity=0.2)\n",
    "fig.update_xaxes(title=\"Depth\")\n",
    "fig.update_yaxes(title=\"Residual\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(x = X[:,3], y = Y - Y_hat,opacity=0.2)\n",
    "fig.update_xaxes(title=\"Table\")\n",
    "fig.update_yaxes(title=\"Residual\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A better way to visualize higher dimensional regression models is to compare the Predicted Value against the Observed value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(x = Y.flatten(), y = Y_hat.flatten(), opacity=0.2)\n",
    "fig.add_trace(go.Scatter(x=[0,20000], y=[0, 20000], name=\"Y_hat=Y\"))\n",
    "fig.update_xaxes(title=\"Y\")\n",
    "fig.update_yaxes(title=\"Y_hat\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we see?  We are under estimating expensive diamonds!  Perhaps we should consider the cut, color, and clarity.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whats Next\n",
    "\n",
    "In the next notebook lecture we will see how to improve this model by applying feature engineering techniques to incorporate categorical data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
